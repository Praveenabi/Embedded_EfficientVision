## CNN Models
- **RegNet** 
    - [Designing Network Design Spaces](https://arxiv.org/pdf/2003.13678)

## Lightweight CNN Models
- **ACNet**
    - [ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks](https://arxiv.org/pdf/1908.03930)
- **MobileNets**
    - [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/pdf/1704.04861)
- **MobileNet V2**
    - [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/pdf/1801.04381)
- **MobileNet V3**
    - [Searching for MobileNetV3](https://arxiv.org/pdf/1905.02244)
- **MobileOne**
    - [MobileOne: An Improved One millisecond Mobile Backbone](https://arxiv.org/pdf/2206.04040)
- **ParCNet**
    - [ParC-Net: Position Aware Circular Convolution with Merits from ConvNets and Transformer](https://arxiv.org/pdf/2203.03952)
- **ShuffleNet**
    - [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](https://arxiv.org/pdf/1707.01083)
- **ShuffleNet V2**
    - [ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design](https://arxiv.org/pdf/1807.11164)
- **RepVGG**
    - [RepVGG: Making VGG-style ConvNets Great Again](https://arxiv.org/pdf/2101.03697)
- **ResNet**
    - [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385)
- **QARepVGG**
    - [Make RepVGG Greater Again: A Quantization-aware Approach](https://arxiv.org/pdf/2212.01593) 


## ViT Models
- **Vanilla ViT** 
    - [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929)
- **Biformer**
    - [BiFormer: Vision Transformer with Bi-Level Routing Attention](https://arxiv.org/pdf/2303.08810)
- **CoAtNet** 
    - [CoAtNet: Marrying Convolution and Attention for All Data Sizes](https://arxiv.org/pdf/2106.04803)
- **CMT** 
    - [CMT: Convolutional Neural Networks Meet Vision Transformers](https://arxiv.org/pdf/2107.06263)
- **CSwin Transformer**
    - [CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows](https://arxiv.org/pdf/2107.00652)
- **DeiT** 
    - [Training data-efficient image transformers & distillation through attention](https://arxiv.org/pdf/2012.12877)
- **DETR**
    - [End-to-end object detection with Transformers](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf)
- **LeViT**
    - [LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference](https://arxiv.org/pdf/2104.01136)
- **LV-ViT**
    - [All Tokens Matter: Token Labeling for Training Better Vision Transformers](https://arxiv.org/pdf/2104.10858)
- **Mask2Former**
    - [Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/pdf/2112.01527)
- **NViT** 
    - [NViT: Vision Transformer Compression and Parameter Redistribution](https://arxiv.org/pdf/2110.04869v1)
- **PoolFormer**
    - [MetaFormer Is Actually What You Need for Vision](https://arxiv.org/pdf/2111.11418)
- **Pyramid Vision Transformer**
    - [Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/pdf/2102.12122)
- **SaViT**
    - [SAViT: Structure-Aware Vision Transformer Pruning via Collaborative Optimization](https://papers.nips.cc/paper_files/paper/2022/file/3b11c5cc84b6da2838db348b37dbd1a2-Paper-Conference.pdf)
- **SegFormer**
    - [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/pdf/2105.15203)
- **Swin Transformer** 
    - [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/pdf/2103.14030)
- **Swin Transformer V2**
    - [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/pdf/2111.09883)
- **ViT-22B**
    - [Scaling Vision Transformers to 22 Billion Parameters](https://arxiv.org/pdf/2302.05442)


## Lightweight ViT Models
- **Conv2Former**
    - [Conv2Former: A Simple Transformer-Style ConvNet for Visual Recognition](https://arxiv.org/pdf/2211.11943)
- **EdgeViTs**
    - [EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers](https://arxiv.org/pdf/2205.03436)
- **EfficientViT**
    - [EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention](https://arxiv.org/pdf/2305.07027)
- **EfficientFormer**
    - [EfficientFormer: Vision Transformers at MobileNet Speed](https://arxiv.org/pdf/2206.01191)
- **EfficientFormerV2**
    - [Rethinking Vision Transformers for MobileNet Size and Speed](https://arxiv.org/pdf/2212.08059)
- **FastViT**
    - [FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization](https://arxiv.org/pdf/2303.14189)
- **Mobile-Former**
    - [Mobile-Former: Bridging MobileNet and Transformer](https://arxiv.org/pdf/2108.05895)
- **MobileViG**
    - [MobileViG: Graph-Based Sparse Attention for Mobile Vision Applications](https://arxiv.org/pdf/2307.00395)
- **MobileViT**
    - [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/pdf/2110.02178)
- **MobileViT V2**
    - [Separable Self-attention for Mobile Vision Transformers](https://arxiv.org/pdf/2206.02680)


## Survey Papers
- [Efficient High-Resolution Deep Learning: A Survey](https://arxiv.org/pdf/2207.13050)
- [A Survey of Algorithmic and Hardware Optimization Techniques for Vision Convolutional Neural Networks on FPGAs](https://cosicdatabase.esat.kuleuven.be/backend/publications/files/journal/3329)